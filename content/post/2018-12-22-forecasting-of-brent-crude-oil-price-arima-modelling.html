---
title: 'Forecasting of Brent Crude Oil Price: ARIMA Modelling '
author: Rami Kh
date: '2018-12-22'
slug: forecasting-of-brent-crude-oil-price-arima-modelling
categories:
  - Time Series Analysis
tags: []
---



<div id="introduction" class="section level2">
<h2>Introduction:</h2>
<p style="text-align: justify;">
Crude oil is almost the most important commodities in the world. It is widely considered to be the lifeblood of our modern life and the crucial driver behind the radical transformations in the global economy over the last decades. The crude oil and its products are almost used in every single industry ranging from aviation to petrochemical. The world currently produces approximately 93 million barrels per day. The market of crude is bigger than all raw metal markets combined at approximate value of 1.7 trillion dollar a year. Given this exceptional importance, the price of crude oil is one of the key determinant of the global economic activity, the international trade, the stock prices, and the macroeconomic policies adopted by the countries worldwide. It has therefore been a subject of a great deal of research. An enormous number of governmental and international agencies, consultancies, hedge funds and think-tanks are always busy designing and evaluating the best approaches to forecast the crude oil prices and to capture its volatile trends. In this project, I will build an econometric model in order to study the dynamics of monthly Brent oil prices for the period from 1987 to 2018 using the <a href="https://ramikh.netlify.com/post/forecasting-of-brent-crude-oil-price-arima-modelling/">Box-Jenkins method</a>. I will use several ARIMA models in order to assess their forecasting performances using the relevant statistical tests and techniques. The data for this project is obtained from the Energy Information Administration (EIA) for the spot price of Brent crude in Europe in American Dollar for the mentioned period.
</p>
<div id="packages-used-in-modelling-and-forecasting" class="section level3">
<h3>Packages used in Modelling and Forecasting</h3>
<pre class="r"><code>library(ggplot2)
library(readxl)
library(urca)
library(tseries)
library(astsa)
library(forecast)</code></pre>
<pre><code>## 
## Attaching package: &#39;forecast&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:astsa&#39;:
## 
##     gas</code></pre>
</div>
</div>
<div id="methodology" class="section level2">
<h2>Methodology</h2>
<p><strong>1- Identification:</strong></p>
<p style="text-align: justify;">
Following the Box-Jenkins method, the first step in building our econometric model should be started with the assessment of the stationarity of the time series under consideration. If the time series proves to be non-stationary, it should be differenced until it becomes stationary.
</p>
<p style="text-align: justify;">
Let us first import our data.
</p>
<pre class="r"><code>brent &lt;- read.csv(&quot;C:/Users/Rami/Desktop/brent.csv&quot;)
head(brent)</code></pre>
<pre><code>##         Date Price
## 1 15/05/1987 18.58
## 2 15/06/1987 18.86
## 3 15/07/1987 19.86
## 4 15/08/1987 18.98
## 5 15/09/1987 18.31
## 6 15/10/1987 18.76</code></pre>
<p style="text-align: justify;">
Since we are going to conduct a time series analysis, we need to transform the dataframe into a time series object first.
</p>
<pre class="r"><code>brent_ts &lt;- ts(brent[,2], start = c(1987,5), frequency = 12)</code></pre>
<p style="text-align: justify;">
Let us then plot our series in order to capture its main charachteristics.
</p>
<pre class="r"><code>ts.plot(brent_ts)</code></pre>
<p><img src="/post/2018-12-22-forecasting-of-brent-crude-oil-price-arima-modelling_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p style="text-align: justify;">
As we can see, our time series is far from being stationary. It has a positive trend in general, however, it includes vloatile periods as well. The price of crude oil has been relatively stable during 1980s and 1990s before increasing drastically starting from 2000 onward. The price has then experienced a remarkable slump in 2009 as a result of the financal crisis. It then took an upward trend before sliding again in 2015 as a result of OPEC policy to restore its market share from shale oil producers.
</p>
<p style="text-align: justify;">
Let us check the autocorrelation plot for the series.
</p>
<pre class="r"><code>par(mfrow = c(2,1))
acf(brent_ts)
pacf(brent_ts)</code></pre>
<p><img src="/post/2018-12-22-forecasting-of-brent-crude-oil-price-arima-modelling_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p style="text-align: justify;">
The series is apparently non-stationarY and this is confirmed by the plots of ACf -as if oil price was stationary, the ACF would decay to zero much faster. The ACF plot above suggests a strong persistence across all lags, that is, a small shock at one lag period will influence the the future predictions of the time series for a long time.
</p>
<p style="text-align: justify;">
Though the series looks non-stationary, it might be a good idea to conduct Augmented Dickey-Fuller Test in order to prove our “visual” conclusion.
</p>
<pre class="r"><code>adf.test(brent_ts)</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  brent_ts
## Dickey-Fuller = -2.2194, Lag order = 7, p-value = 0.4845
## alternative hypothesis: stationary</code></pre>
<p style="text-align: justify;">
The p-value is larger than 0.05 and thus we fail to reject the null hypothesis which states that the time series includes a unit root. This indicates that the time series should be differenced in order to transform it into a stationary process.
</p>
<pre class="r"><code>diff_series &lt;- diff(brent_ts)</code></pre>
<pre class="r"><code>layout(matrix(c(1,2,3,3), 2, 2, byrow = TRUE))
b &lt;- acf(diff_series)
c &lt;- pacf(diff_series)
a &lt;- ts.plot(diff_series)</code></pre>
<p><img src="/post/2018-12-22-forecasting-of-brent-crude-oil-price-arima-modelling_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p style="text-align: justify;">
We can see from the plot above that the series istelf and ACF look similar to a plot for the white noise process. The ACF drops to zero much faster comparing to the one for non-stationary series. The plot also shows that only two autocorrelations lie outside 95% confidence limits.
</p>
<pre class="r"><code>adf.test(diff_series)</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  diff_series
## Dickey-Fuller = -7.7406, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary</code></pre>
<p style="text-align: justify;">
And when we apply Augmented Dickey-Fuller Test on the differenced series, we got p-value of 0.01, which suggests that the series is stationary.
</p>
<p><strong>2- Modelling:</strong></p>
<p style="text-align: justify;">
The plots for ACF and PACF suggests that the sutiable model for fitting the data could be ARIMA(1,1,0), however, for the purpose of this project, I’m going to simulate several models with different orders of p and q. I will also leave the task of fitting one of the models for R by using <strong>auto.arima()</strong> function. I will then compare AICs for the models and pick up two or three of them for forecasting.
</p>
<p style="text-align: justify;">
Before start fitting the models, I will split my data into training set and test set where the latter includes only the observations for the last 12 months. Splitting the data into training and test sets is one of the widely used practices when it comes to constructing predictive models. Using this technique will help us evaluating the accuracy of forecasting power by measuring how well the model is likely to forecast on new data that has not been part of fitting the model.
</p>
<pre class="r"><code>training &lt;- subset(brent_ts, end = 367)
test &lt;- subset(brent_ts, end = 368)</code></pre>
<p><strong>- Model1: ARIMA (1, 1, 1)</strong></p>
<pre class="r"><code>Model1 &lt;- Arima(training, order = c(1,1,1), include.drift = TRUE)
summary(Model1)</code></pre>
<pre><code>## Series: training 
## ARIMA(1,1,1) with drift 
## 
## Coefficients:
##          ar1      ma1   drift
##       0.3948  -0.0215  0.1288
## s.e.  0.1117   0.1187  0.3489
## 
## sigma^2 estimated as 17.24:  log likelihood=-1038.95
## AIC=2085.91   AICc=2086.02   BIC=2101.52
## 
## Training set error measures:
##                        ME     RMSE      MAE        MPE     MAPE      MASE
## Training set 0.0002437218 4.129523 2.838451 -0.2315886 6.742997 0.2409493
##                      ACF1
## Training set 0.0004837868</code></pre>
<p><strong>- Model2: ARIMA (1,1,0)</strong></p>
<pre class="r"><code>Model2 &lt;- Arima(training, order = c(1,1,0), include.drift = TRUE)
summary(Model2)</code></pre>
<pre><code>## Series: training 
## ARIMA(1,1,0) with drift 
## 
## Coefficients:
##          ar1   drift
##       0.3763  0.1284
## s.e.  0.0484  0.3460
## 
## sigma^2 estimated as 17.19:  log likelihood=-1038.97
## AIC=2083.94   AICc=2084.01   BIC=2095.65
## 
## Training set error measures:
##                       ME   RMSE      MAE        MPE     MAPE      MASE
## Training set 0.000365657 4.1297 2.836782 -0.2352411 6.738997 0.2408077
##                      ACF1
## Training set -0.002996357</code></pre>
<p><strong>- Model3: ARIMA (2,1,2)</strong></p>
<pre class="r"><code>Model3 &lt;- Arima(training, order = c(2,1,2), include.drift = TRUE)
summary(Model3)</code></pre>
<pre><code>## Series: training 
## ARIMA(2,1,2) with drift 
## 
## Coefficients:
##          ar1      ar2      ma1     ma2   drift
##       1.4048  -0.5487  -1.0483  0.2030  0.1174
## s.e.  0.1203   0.1115   0.1377  0.1387  0.2317
## 
## sigma^2 estimated as 17.02:  log likelihood=-1035.62
## AIC=2083.24   AICc=2083.47   BIC=2106.65
## 
## Training set error measures:
##                        ME     RMSE      MAE        MPE     MAPE      MASE
## Training set -0.001586251 4.091538 2.830133 -0.3881241 6.705963 0.2402433
##                     ACF1
## Training set 0.001115057</code></pre>
<p><strong>- Model4: ARIMA (2,1,0)</strong></p>
<pre class="r"><code>Model4 &lt;- Arima(training, order = c(2,1,0), include.drift = TRUE)
summary(Model4)</code></pre>
<pre><code>## Series: training 
## ARIMA(2,1,0) with drift 
## 
## Coefficients:
##          ar1     ar2   drift
##       0.3723  0.0107  0.1295
## s.e.  0.0523  0.0522  0.3497
## 
## sigma^2 estimated as 17.24:  log likelihood=-1038.95
## AIC=2085.9   AICc=2086.01   BIC=2101.51
## 
## Training set error measures:
##                         ME     RMSE      MAE        MPE    MAPE      MASE
## Training set -0.0001757545 4.129462 2.838979 -0.2319836 6.74433 0.2409942
##                     ACF1
## Training set 0.001734294</code></pre>
<p><strong>- Model5: Auto Arima</strong></p>
<pre class="r"><code>Model5 &lt;- auto.arima(training)
summary(Model5)</code></pre>
<pre><code>## Series: training 
## ARIMA(1,1,0)(0,0,2)[12] 
## 
## Coefficients:
##          ar1    sma1     sma2
##       0.3942  0.0778  -0.1522
## s.e.  0.0483  0.0522   0.0514
## 
## sigma^2 estimated as 16.7:  log likelihood=-1033.5
## AIC=2075.01   AICc=2075.12   BIC=2090.62
## 
## Training set error measures:
##                      ME     RMSE      MAE        MPE     MAPE      MASE
## Training set 0.08280644 4.064694 2.796978 0.07071734 6.686554 0.2374287
##                      ACF1
## Training set -0.006713012</code></pre>
<p style="text-align: justify;">
<strong>Note:</strong> When we apply <strong>auto.arima()</strong> function on data, it chooses the best model to be ARIMA(1,1,0)(0,0,2)[12], which means that the function believes that the series of crude oil price includes a seasonal component.
</p>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<pre><code>&lt;th class=&quot;tg-0pky&quot;&gt;Model&lt;/th&gt;
&lt;th class=&quot;tg-0pky&quot;&gt;ARIMA(1,1,1)&lt;/th&gt;
&lt;th class=&quot;tg-0pky&quot;&gt;ARIMA(1,1,0)&lt;/th&gt;
&lt;th class=&quot;tg-0pky&quot;&gt;ARIMA(2,1,2)&lt;/th&gt;
&lt;th class=&quot;tg-0pky&quot;&gt;ARIMA(2,1,0)&lt;/th&gt;
&lt;th class=&quot;tg-0pky&quot;&gt;ARIMA(1,1,0)(0,0,2)&lt;/th&gt;</code></pre>
</tr>
<tr>
<pre><code>&lt;td class=&quot;tg-0pky&quot;&gt;AIC&lt;/td&gt;
&lt;td class=&quot;tg-c3ow&quot;&gt;2086.02&lt;/td&gt;
&lt;td class=&quot;tg-c3ow&quot;&gt;2084.01&lt;/td&gt;
&lt;td class=&quot;tg-c3ow&quot;&gt;2083.47&lt;/td&gt;
&lt;td class=&quot;tg-c3ow&quot;&gt;2086.01&lt;/td&gt;
&lt;td class=&quot;tg-c3ow&quot;&gt;2075.12&lt;/td&gt;</code></pre>
</tr>
</table>
<p style="text-align: justify;">
We can see in table above that the model which has been choosen by auto.arima() has the lowest AIC. Moreover, models ARIMA (1,1,0) and ARIMA (2,1,2) have lower AIC comparing to ARIMA (1,1,1) and ARIMA (2,1,0).
</p>
<p><strong>3- Residual Analysis:</strong></p>
<p style="text-align: justify;">
In addition to the comparison of AICs for our models, we need to make sure that the residuals of each one meet the assumptions of randomness, normal distribution and uncorrelation.
</p>
<pre class="r"><code>checkresiduals(Model1)</code></pre>
<p><img src="/post/2018-12-22-forecasting-of-brent-crude-oil-price-arima-modelling_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre><code>## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(1,1,1) with drift
## Q* = 45.47, df = 21, p-value = 0.001502
## 
## Model df: 3.   Total lags used: 24</code></pre>
<pre class="r"><code>checkresiduals(Model2)</code></pre>
<p><img src="/post/2018-12-22-forecasting-of-brent-crude-oil-price-arima-modelling_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre><code>## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(1,1,0) with drift
## Q* = 45.274, df = 22, p-value = 0.002449
## 
## Model df: 2.   Total lags used: 24</code></pre>
<pre class="r"><code>checkresiduals(Model3)</code></pre>
<p><img src="/post/2018-12-22-forecasting-of-brent-crude-oil-price-arima-modelling_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre><code>## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(2,1,2) with drift
## Q* = 40.391, df = 19, p-value = 0.002907
## 
## Model df: 5.   Total lags used: 24</code></pre>
<pre class="r"><code>checkresiduals(Model4)</code></pre>
<p><img src="/post/2018-12-22-forecasting-of-brent-crude-oil-price-arima-modelling_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre><code>## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(2,1,0) with drift
## Q* = 45.537, df = 21, p-value = 0.001472
## 
## Model df: 3.   Total lags used: 24</code></pre>
<pre class="r"><code>checkresiduals(Model5)</code></pre>
<p><img src="/post/2018-12-22-forecasting-of-brent-crude-oil-price-arima-modelling_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre><code>## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(1,1,0)(0,0,2)[12]
## Q* = 30.429, df = 21, p-value = 0.08369
## 
## Model df: 3.   Total lags used: 24</code></pre>
</div>
