---
title: 'Estimation of the Aggregate Consumption Equation: Linear Regression Analysis'
author: Rami Kh
date: '2018-12-13'
slug: estimation-of-the-consumption-equation-linear-regression-analysis
categories: []
tags: []
---


##Introduction

<p style='text-align: justify;'> A simple linear regression is a statistical model that allows us to analyse the relationship between a dependent variable (response) and independent variable (predictor). The model is considered as one of the most popular mathematical techniques to predict the value of the outcome variable based on one or more input predictor variables. It has been used for ages as a simple and straightforward approach in studying the linear relationships in a wide spectrum of fields ranging from economics to biology and it takes the following formula: </p>


<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true">
    <mlabeledtr>
      <mtd id="mjx-eqn-1">
        <mtext></mtext>
      </mtd>
      <mtd>
        <mi>Y</mi>
        <mo>=</mo>
        <msub>
          <mi>&#x03B2;<!-- ?? --></mi>
          <mn>0</mn>
        </msub>
        <mo>+</mo>
        <msub>
          <mi>&#x03B2;<!-- ?? --></mi>
        </msub>
        <mi>X</mi>
        <mo>+</mo>
        <mi>&#x03F5;<!-- ?? --></mi>
      </mtd>
    </mlabeledtr>
  </mtable>
</math>

Where: 

Y  represents the dependent (response) variable.

X  represents the independent (predictor) variable.

**${\beta}_0$** represents the intercept (the value of Y when X = Zero).

**${\beta}_1$** is the coefficient (slope term) representing the linear relationship.

**$\epsilon$** represents the error term (Normally distributed/Constant Variance).



**The simple linear regression is established upon the following assumptions:**

- **Linear:** the relationship between $Y$ and $X$ is linear.
- **Independent:** the errors $\epsilon$ are independent.
- **Normal:** the errors $\epsilon$ are noramlly distributed.
- **Equal Variance:** the variance of $Y$ is the same at each value of $X$.


##Least Squares Method

<p style='text-align: justify;'>If we know the population parameters <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>&#x03B2;<!-- ?? --></mi>
    <mn>0</mn>
  </msub>
</math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>&#x03B2;<!-- ?? --></mi>
    <mn>1</mn>
  </msub>
</math>, we could use the simple linear regression model where 

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtext>E</mtext>
  <mo stretchy="false">[</mo>
  <msub>
    <mi>Y</mi>
    <mi>i</mi>
  </msub>
  <mo>&#x2223;<!-- ∣ --></mo>
  <msub>
    <mi>X</mi>
    <mi>i</mi>
  </msub>
  <mo>=</mo>
  <msub>
    <mi>x</mi>
    <mi>i</mi>
  </msub>
  <mo stretchy="false">]</mo>
  <mo>=</mo>
  <msub>
    <mi>&#x03B2;<!-- β --></mi>
    <mn>0</mn>
  </msub>
  <mo>+</mo>
  <msub>
    <mi>&#x03B2;<!-- β --></mi>
    <mn>1</mn>
  </msub>
  <msub>
    <mi>x</mi>
    <mi>i</mi>
  </msub>
  <mo>.</mo>
</math

<p style='text-align: justify;'>However, in reality we almost never have the parameters and thus we should estimate the value of $Y$
at every given value of $X$</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow class="MJX-TeXAtom-ORD">
    <mover>
      <mi>y</mi>
      <mo stretchy="false">&#x005E;<!-- ^ --></mo>
    </mover>
  </mrow>
  <mo>=</mo>
  <msub>
    <mrow class="MJX-TeXAtom-ORD">
      <mover>
        <mi>&#x03B2;<!-- β --></mi>
        <mo stretchy="false">&#x005E;<!-- ^ --></mo>
      </mover>
    </mrow>
    <mn>0</mn>
  </msub>
  <mo>+</mo>
  <msub>
    <mrow class="MJX-TeXAtom-ORD">
      <mover>
        <mi>&#x03B2;<!-- β --></mi>
        <mo stretchy="false">&#x005E;<!-- ^ --></mo>
      </mover>
    </mrow>
    <mn>1</mn>
  </msub>
  <mi>x</mi>
  <mo>.</mo>
</math>

where: 

$\hat{y}$ refers to the mean value of $Y$ for a given value of $X$


<p style='text-align: justify;'>**The Least Squares Mehtod** is the statistical approach that can be used to minimize the sum of all the squared distances between the observed $y$ and the predicted $\hat{y}$ at each given value of $x$</p>

![](/images/model.png)

<p style='text-align: justify;'>Using the image above, **Least Squares Method** is a mathemtical technique through which we can find the value of ${\beta}_0$ (intercept) and ${\beta}_1 (slope)$ in order to draw a line that fits the data in a way minimizing the squared distances between the observed $y$ and the predicted $\hat{y}$, namely the squared distances between the the red points and the blue line (squared errors). </p> 


$$
{\beta}_1 = \frac{\sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^{n}(x_i - \bar{x})^2}
$$
and the intercept is: 

$$
{\beta}_0 = \bar{y} - {\beta}_1 \bar{x}
$$
 
##Estimating the Consumption Equation


<p style='text-align: justify;'> In this post, I will use the simple linear regression to examine whether there is any relationship between the gross national disposable income (GNDI) per capita and the aggregate consumption per capita in the Palestinian Territories. I'm going to use a data extrcted from the Palestinian Central Bureau of Statistics covering the period from 1994 to 2016 using 2004 as a base year. The indepenent variable (predictor) will be the gross national disposable income per capita while the dependent variable (response) will be the the aggregate consumption per capita. The gross national disposable income refers to is the sum of the disposable incomes of all residents, and it measures the income available to the nation for final consumption and gross (or net) saving including the imcomes comes from external sources such as remittances and international aids.</p>

<p style='text-align: justify;'>Let us first load the packages needed for analysis. </p>

```{r message=FALSE, warning=FALSE}
library(readxl)
library(ggplot2)
library(PerformanceAnalytics)
library(broom)
library(GGally)
library(modelr)
library(lars)
```


<p style='text-align: justify;'>Let us now import our data. </p>

```{r}
consumption <- read_excel("C:/Users/Rami/Desktop/gndi-con.xlsx")

consumption
```


##Exploratory Analysis


<p style='text-align: justify;'>Before building our model, it would be a good start to get an initial snapshot of our data.</p>

```{r}
summary(consumption)
```


<p style='text-align: justify;'>Let us draw a scatter plot for data.</p>

```{r message=FALSE, warning=FALSE}

p1 <- ggplot(consumption, aes(x=consumption$GNDIPC, y=consumption$ConsumptionPC)) +
  geom_point(size= 2) + theme_minimal()

p1
```


<p style='text-align: justify;'>The scatter plot above suggests that there is a poisitive linear relationship existed between the two variables; the aggregate consumption per capita tends to increase along with the increase in the gross disposable income per capita.</p>

<p style='text-align: justify;'>If we want to quantify the relationship's strength between the two variables, we can use the correlation formula: </p>

$$
r = \frac{\sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^{n}(x_i - \bar{x})^2  (y_i - \bar{y})^2}
$$

```{r}
#Transforming the variables into integers

consumption$`GNDI PC` <- as.integer(consumption$GNDIPC)
consumption$`Consumption PC` <- as.integer(consumption$ConsumptionPC)
```

```{r}
cor(consumption$GNDIPC, consumption$ConsumptionPC)
```

```{r}
chart.Correlation(consumption[, c("GNDIPC", "ConsumptionPC")])
```

<p style='text-align: justify;'>The plot above shows that the relationship between the two variables is fairly strong. Now we are going to build our model.</p>


##Building the Linear Model

<p style='text-align: justify;'>We can build our model using **'lm'** function in R which stands for "Linear Model". This function will produce the best-fit linear relationship by minimizing the least squares. In other words, the **'lm'** funtion will enable us to estimate the value of coefficents (Slope and Intercept) in addition to the fitted values of $y$ for each value of $x$.</p>


```{r}
model <- lm(consumption$ConsumptionPC ~ consumption$GNDIPC)
```

```{r}
summary(model)
```


##Assessing Coefficients 

<p style='text-align: justify;'>Given the estimates that we got from the model, we can now establish our fitted regression:</p>

$$Y = 539 + 0.656X + e$$

<p style='text-align: justify;'>And we can get a tidy version of our model by using function "tidy" from broom package. </p>

```{r}
tidy(model)
```


<p style='text-align: justify;'>The table above shows that our intercept estimate is 539, so when gross national income per capita  is zero, we can expect aggregate consumption per capita to be 539. The table also shows that the slope estimate is 0.656, which means that for every increase of one dollar in gross national disposable income per capita, we expect the aggregate consumption per capita to increase by $0.656. But are these coefficients statistically significant? We can examine this by computing the standard error at 95% confidence interval for the cofficients.</p>

```{r}
confint(model)
```

<p style='text-align: justify;'>The table above shows that our 95% confidence interval for ${\beta}_1$ is [0.419,0.892]. Since zero is not included in the interval, we can conclude that as gross national disposable income per capita increases by $1, the aggregate consumption per capita is going to increase by 0.419-0.892 dollars. </p>


##Assessing the Godness-of-Fit


<p style='text-align: justify;'>We should ask ouselves now: **How well the model fits our data?**. In order to answer this question, we are going to compute two things: </p>

1- Residual Standard Error (RSE).

2- R-Squared ($R^2$).


<p style='text-align: justify;'>**Residual Standard Error (RSE)**: refers to the estimate of the standard deviation of $\epsilon$. It provides us with the average distance the observed values deviate from the regression line measured by the units of response variable. RSE takes the following formula: </p>

$$RSE = \sqrt{\frac{1}{n-2}\sum^n_{i=1}(y_i - \hat{y}_i)^2}$$


<p style='text-align: justify;'>We can get the value of RSE in R using sigma function from broom package.</p>

```{r}
sigma(model)
```


<p style='text-align: justify;'>RSE value of 112.60 means that the actual aggregate consumption per capita will deviate from the true regression line by approximately $112.60, on average. As RSE is an absolute value that does not explain so much by itself, it might be a good idea to comapre it with the mean of the aggregate consumption per capita. </p>

```{r}
sigma(model)/mean(consumption$ConsumptionPC)
```


<p style='text-align: justify;'>We got a percentage error of 6.1% when we compare the RSE to the mean of the response variable. This percentage suggests that our model fits the data well as the deviation of the response variable from the regression line is not so big. </p>


<p style='text-align: justify;'>**R-Squared ($R^2$)**: expresses the proportion of the variance in the depdendent variable (response ) that is "explained" by the model. That is, the variation in aggregate consumption per capita due to the changes in the gross national disposable income per capita. In order to fully understand the logic behind $R^2$, let us first introduce three important formulas. </p>

<p style='text-align: justify;'>**1- Sum of Squares Total**: refers to the **total** variation of the observed values of $y$</p>

$$\text{SST} = \sum_{i=1}^{n}(y_i - \bar{y})^2$$

<p style='text-align: justify;'>**2- Sum of Squares Regression**: refers to the **explained** variation of the observed values of $y$</p>

$$\text{SSR} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$$

<p style='text-align: justify;'>**3- Sum of Squares Error**: refers to the **unexplained** variation of the observed values of $y$ </p>

$$\text{SSE} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

<p style='text-align: justify;'>Since $R^2$ measures the variation in the response variable that is **explained** by the model, it thus takes the following formula:</p>

$$R^2 =  \frac{SSR}{SST}= \frac{\sum^n_{i=1}(y_i-\hat{y}_i)^2}{\sum^n_{i=1}(y_i-\bar{y}_i)^2}$$

<p style='text-align: justify;'>$R^2$ for our model is equal to 0.6129, which means that 61% of the variability in the aggregate consumption per capita can be explained by the gross national disposable income per capita. </p>


##Residual Analysis


<p style='text-align: justify;'>Only part of the variation in the dependent variable will be explained by the values of the independent variable SSR/SST (which is equal to 0.61 in our model). The variation left unexplained is due to the error SSE/SST, and this is why we should also conduct an analysis on the residuals of our model. The main issue here is to make sure that the residuals are following the assumptions of the linear regression. The residuals analysis can be made through 4-plots grid in R.</p> 


<p style='text-align: justify;'>**Residuals Vs. Fitted Values**: This plot shows if the residuals have a non-linear pattern. If the residuals spread around the horizontal lines without showing specific patter, this can be deemed as a sign of the model's linearity. </p>

<p style='text-align: justify;'>**Normal Q-Q**: This plot is concerned with the normal distribution of the residuals. If the residuals follow a straight line and do not deviate far from the line, this should be a good indication that the residulas are normally distributed. </p>

<p style='text-align: justify;'>**Scale-Location**: This plot shows if the residuals are spread equally along with the fitted values. This plot enables us to make sure that the assumption of homoscedasticity (equall variance of residuals). If the residuals are randomly spread, this should be a good sign that the residuals have the same variance for all values of the predictor variable. </p>

<p style='text-align: justify;'>**Residuals Vs. Leverage**: This plot enables us to detect the influential cases. The data might include extreme values, however; these values can be with no influential impact on our regression line. On the other hand, some values that lay wihtin the range of the data might be very influential. If the cases are outside of the Cook’s distance (namely outside of a dashed line), the cases  are influential to the regression results. </p>

```{r}
par(mfrow = c(2, 2))
plot(model)
```

<p style='text-align: justify;'>The four plots of our model show that the residuals almost have no specific pattern around the horizontal line, suggesting that the relationship between the response and the predictor variables is linear. The plots also indicate that most residuals fall on the Q-Q line, suggesting that the residuals are normally distributed. However, Residuals Vs. Leverage plot clearly shows that data point No. 15 is an outlier that has a large impact on our model. This can be seen in all other three plots where point 15 deviates far from the other data points. </p>

<p style='text-align: justify;'>Let us dig deeper in our data to see how point 15 had such impact on our model. I'm going to extract the observed and fitted values of response variable in addition to the residuals. </p>

```{r}
as.data.frame(consumption$ConsumptionPC)
(model$fitted.values)
(model$residuals)
```

<p style='text-align: justify;'>As we can see, though the obsererved value of response variable at point 15 equals to 1728.48 (which lies within a reasonable range of our data), it's fitted value equals to 2031.257. That is, 1728.48 (observed) - 2031.257 (fitted) = -302.777345 (residual) whcih looks high comparing to other residuals for other fitted values in our model. This confirms the wisdom that some values that look "reasonable" wihtin the range of the data might be very influential after fitting the model. </p>


##Excluding the Outlier

<p style='text-align: justify;'>Excluding the outlier from the model might not be the best approach to deal with its impact, however, for the purpose of this project, I'm going to ommit the outlier from the data in order to examine how this step can aggect the model's quality. </p>

<p style='text-align: justify;'>Let us import our new data without outlier. </p>

```{r}
consumption_new <- read_excel("C:/Users/Rami/Desktop/pcon.xlsx")
```

```{r}
#Transforming the variables into integers

consumption_new$`GNDI PC` <- as.integer(consumption_new$GNDIPC)
consumption_new$`Consumption PC` <- as.integer(consumption_new$ConsumptionPC)
```

```{r}
chart.Correlation(consumption_new[, c("GNDIPC", "ConsumptionPC")])
```

<p style='text-align: justify;'>We can notice that the exclusion of the outlier results in increasing the correlation between the independent and the dependent variables from 0.78 to 0.89. In other words, omiting the impact of the outlier makes the positive relationship between the two variables much stronger. </p>

<p style='text-align: justify;'>Let us now build a new model and assess the coefficients and the godness of fit. </p>

```{r}
model2 <- lm(consumption_new$ConsumptionPC ~ consumption_new$GNDIPC)
summary(model2)
```

<p style='text-align: justify;'>The new model after excluding the outlier is the following: </p>

$$Y = 237 + 0.829X + e$$

<p style='text-align: justify;'>We can see that the slope of the regression line has been increased from 0.656 to 0.829, which means that for every increase of one dollar in gross national disposable income per capita, we expect the aggregate consumption per capita to increase by $0.829, on average. </p>

```{r}
sigma(model2)
sigma(model2)/mean(consumption_new$ConsumptionPC)
```

<p style='text-align: justify;'>Omitting the outlier has also resulted in decreasing the Residual Standard Error from 112.60 in the previous model to 85.68 in the current one. $R^2$ has also increased to 0.78, suggesting that the model without outlier has much stronger explantory power. </p>


##Residual Analysis


<p style='text-align: justify;'>We will now examine how the residuals behave after the exclusion of the outlier from the model using the grid of 4-plots used previously. </p>

```{r}
par(mfrow = c(2, 2))
plot(model2)
```


<p style='text-align: justify;'>We can clearly see from the plots above that the exclusion of the outiler has improved the performance of the residuals. The plot of Residuals Vs. Fitted values suggests that the relationship between the dependent variable and the independent variable is linear. The Q-Q plot shows that the residuals are normally distributed with no significant deviations from the line. The plot of scale-location indictaes that the residulas are not spread equally along the range of predictors as they look somewhat heteroscedastic in the midlle and the end of the range. However, it could be quite difficult to confirm such conclusion with samll set of data as the one under consideration. </p>

##Making Predictions 

<p style='text-align: justify;'>One of the advantages of the linear regression lies in its ability to be a predictive model. However, when we try to use the regression to make predictions, we should take into account two concepts, the first one is **interpolation** and  the second one is **extrapolation**. The interpolation refers to the process where the prediction is made upon a data point that is not an observed value of the independent variable bu it lies within the data range. The extrapolation refers to the process where the prediction is made based on a point outside the data range. </p>




